{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chavvi/anaconda2/lib/python2.7/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 10831749999450280044\n",
      ", name: \"/device:GPU:0\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 7468364596\n",
      "locality {\n",
      "  bus_id: 1\n",
      "  links {\n",
      "  }\n",
      "}\n",
      "incarnation: 2321053522577392011\n",
      "physical_device_desc: \"device: 0, name: GeForce GTX 1070, pci bus id: 0000:01:00.0, compute capability: 6.1\"\n",
      "]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "inception_v3 (Model)         (None, None, None, 2048)  21802784  \n",
      "=================================================================\n",
      "Total params: 21,802,784\n",
      "Trainable params: 21,768,352\n",
      "Non-trainable params: 34,432\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())\n",
    "from keras.applications.inception_v3 import InceptionV3\n",
    "from keras.preprocessing import image\n",
    "from keras.models import Model\n",
    "from keras import backend as K\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "from keras.layers import Input, Dense, Dropout, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D, GlobalAveragePooling2D\n",
    "from keras.models import Sequential, Model\n",
    "from keras.optimizers import Adam,SGD\n",
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image ,ImageOps \n",
    "import matplotlib.pyplot as plt\n",
    "sess = tf.Session(config=tf.ConfigProto(log_device_placement=True))\n",
    "inception_v3 = InceptionV3(include_top=False, weights='imagenet', input_tensor=None, pooling=None)\n",
    "model = Sequential()\n",
    "model.add(inception_v3)\n",
    "print (model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_image(imgpath):\n",
    "    desired_size = 299\n",
    "    im_pth = imgpath\n",
    "    im = Image.open(im_pth)\n",
    "    old_size = im.size  # old_size[0] is in (width, height) format\n",
    "    ratio = float(desired_size)/max(old_size)\n",
    "    new_size = tuple([int(x*ratio) for x in old_size])\n",
    "    # use thumbnail() or resize() method to resize the input image\n",
    "    # thumbnail is a in-place operation\n",
    "    # im.thumbnail(new_size, Image.ANTIALIAS)\n",
    "    im = im.resize(new_size, Image.ANTIALIAS)\n",
    "    # create a new image and paste the resized on it\n",
    "    new_im = Image.new(\"RGB\", (desired_size, desired_size))\n",
    "    new_im.paste(im, ((desired_size-new_size[0])//2,\n",
    "                        (desired_size-new_size[1])//2))\n",
    "    read_img = np.array(new_im)\n",
    "    mask = read_img!=0\n",
    "    read_img[mask] = 255\n",
    "    return read_img\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getFeatures(X):\n",
    "    tans = model.predict(X)\n",
    "    ans = [np.mean(avgpool, axis=(0,1)) for avgpool in tans]\n",
    "    return ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'os' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-e6c86b312d1a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0maction_folder\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirectory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0maction_num\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfindall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mr'\\d+'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maction_folder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname_process\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction_folder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'os' is not defined"
     ]
    }
   ],
   "source": [
    "def name_process(name):\n",
    "    if(name[0]=='_'):\n",
    "        return True\n",
    "    elif (name[0]=='.'):\n",
    "        return True\n",
    "    return False\n",
    "directory = 'VideosCrop'\n",
    "X = []\n",
    "y = []\n",
    "for action_folder in os.listdir(directory):\n",
    "    action_num = re.findall(r'\\d+',action_folder)[0]\n",
    "    if name_process(action_folder):\n",
    "        continue\n",
    "    path = directory + '/' + action_folder\n",
    "    for frame_folder_num, frame_folder in enumerate(os.listdir(path)):\n",
    "        one_video = []\n",
    "        if name_process(frame_folder):\n",
    "            continue\n",
    "        frame_folder_path = path + '/' + frame_folder\n",
    "        for frame in os.listdir(frame_folder_path):\n",
    "            if name_process(frame):\n",
    "                continue\n",
    "            img_path = frame_folder_path +'/' + frame\n",
    "            if(not frame.endswith('.png')):\n",
    "                continue\n",
    "            reframe = get_image(img_path) \n",
    "            one_video.append(reframe)\n",
    "            features = np.zeros((60,2048))\n",
    "        features[:np.array(one_video).shape[0],:] = getFeatures(np.array(one_video)/255)\n",
    "        print (np.array(features).shape)\n",
    "        X.append(features)\n",
    "        y.append([action_num-1]*features.shape[0])\n",
    "        print(np.array(one_video).shape)\n",
    "print (np.array(X).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "from keras.utils import to_categorical\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, Input, Dropout, LSTM, Activation\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence\n",
    "\n",
    "\n",
    "encoded_y = to_categorical(y,20)\n",
    "print (encoded_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20, 60, 2048)\n"
     ]
    }
   ],
   "source": [
    "def build_model(maxlen):\n",
    "    input_videos = Input((maxlen,2048), dtype='float32')\n",
    "    X = LSTM(128, return_sequences=True)(input_videos)\n",
    "    X = Dropout(0.5)(X)\n",
    "    X = LSTM(128, return_sequences=False)(X)\n",
    "    X = Dropout(0.5)(X)\n",
    "    X = Dense(20)(X)\n",
    "    X = Activation('softmax')(X)\n",
    "    \n",
    "    model = Model(inputs=input_videos, outputs=X)\n",
    "    \n",
    "    return model\n",
    "print np.array(one_class).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "20/20 [==============================] - 4s 221ms/step - loss: 0.5368 - acc: 0.7500\n",
      "Epoch 2/10\n",
      "20/20 [==============================] - 1s 40ms/step - loss: 0.1855 - acc: 1.0000\n",
      "Epoch 3/10\n",
      "20/20 [==============================] - 1s 41ms/step - loss: 0.0029 - acc: 1.0000\n",
      "Epoch 4/10\n",
      "20/20 [==============================] - 1s 40ms/step - loss: 1.1712e-04 - acc: 1.0000\n",
      "Epoch 5/10\n",
      "20/20 [==============================] - 1s 40ms/step - loss: 1.1142e-04 - acc: 1.0000\n",
      "Epoch 6/10\n",
      "20/20 [==============================] - 1s 40ms/step - loss: 3.0704e-05 - acc: 1.0000\n",
      "Epoch 7/10\n",
      "20/20 [==============================] - 1s 41ms/step - loss: 4.1268e-05 - acc: 1.0000\n",
      "Epoch 8/10\n",
      "20/20 [==============================] - 1s 40ms/step - loss: 1.4472e-05 - acc: 1.0000\n",
      "Epoch 9/10\n",
      "20/20 [==============================] - 1s 40ms/step - loss: 1.2031e-05 - acc: 1.0000\n",
      "Epoch 10/10\n",
      "20/20 [==============================] - 1s 40ms/step - loss: 1.2568e-05 - acc: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fb37795af50>"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "lstm_model = build_model(60)\n",
    "lstm_model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "lstm_model.fit(np.array(X), encoded_y, epochs = 10, batch_size = 10, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "print (03)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
